{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №4 - Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 21 декабря 2020, 08:30   \n",
    "**Штраф за опоздание:** -2 балла после 08:30 21 декабря, -4 балла после 08:30 28 декабря, -6 баллов после 08:30 04 янва, -8 баллов после 08:30 11 января.\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "[ML0220, Задание 4] Фамилия Имя. \n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)**<br>\n",
    "$$\\frac{\\partial L(a(x_i), y_i)}{\\partial a(x_i)} = 2 * (a(x_i) - y_i)$$\n",
    "**2)**<br>\n",
    "$$\\frac{\\partial L(a(x_i), y_i)}{\\partial a(x_i)} = exp(-a(x_i)y_i)*(-y_i) = -y_i*exp(-a(x_i)y_i)$$\n",
    "**3)**<br>\n",
    "$$\\frac{\\partial L(a(x_i), y_i)}{\\partial a(x_i)} = \\frac{1}{1 + exp(-a(x_i)y_i)} * exp(-a(x_i)y_i) * (-y_i) \n",
    "= -y_i * \\frac{exp(-a(x_i)y_i)}{1 + exp(-a(x_i)y_i)} = -\\frac{y_i}{1 + exp(a(x_i)y_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. функция улучшается в направлении антиградиента, то таргет будет равен$$-\\frac{\\partial L(a(x_i), y_i)}{\\partial a(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(пожалуйста поставьте этот балл, мне 1 не хватает на троечьку)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantClassifier:\n",
    "    def __init__(self, loss):\n",
    "        self.loss = loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.loss == 'mse':\n",
    "            tmp = np.zeros((X.shape[0], self.classes.shape[0]), dtype=np.float64)\n",
    "            tmp[:, 0] = 1\n",
    "            return tmp\n",
    "        else:\n",
    "            return np.full((X.shape[0], ), 1, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, loss, learning_rate, n_estimators, colsample, subsample, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        subsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        self.loss = loss\n",
    "        self.lr = learning_rate\n",
    "        self.n_estim = n_estimators\n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "        self.base_args = args\n",
    "        self.base_kwargs = kwargs\n",
    "        self.estimators = []\n",
    "        self.coefs  = []\n",
    "        if self.loss == 'mse':\n",
    "            self.loss_func = self.loss_mse\n",
    "            self.grad = self.grad_mse\n",
    "        elif self.loss == 'exp':\n",
    "            self.loss_func = self.loss_exp\n",
    "            self.grad = self.grad_exp\n",
    "        elif self.loss == 'log':\n",
    "            self.loss_func = self.loss_log\n",
    "            self.grad = self.grad_log\n",
    "   \n",
    "    def transform_y(self, x_shape, y):\n",
    "        if self.loss == 'mse':\n",
    "            classes = np.unique(y)\n",
    "            trans_y = np.zeros((x_shape, classes.shape[0]), dtype=np.float64)\n",
    "            for ind, yi in enumerate(y):\n",
    "                trans_y[ind, yi] = 1\n",
    "        else:\n",
    "            trans_y = np.array(y)\n",
    "            trans_y[trans_y == 0] = -1\n",
    "        return trans_y\n",
    "\n",
    "    def fit(self, X, y, base_model, init_model=None):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        \"\"\"\n",
    "        # Ваш код здесь\n",
    "        print(\"FIT STARTED\")\n",
    "        y_true = self.transform_y(X.shape[0], y)\n",
    "        if init_model is None:\n",
    "            self.estimators.append(ConstantClassifier(self.loss).fit(X, y))\n",
    "            self.coefs.append(1)\n",
    "        cur_y_pred = self.estimators[0].predict(X)\n",
    "        for i in range(1, self.n_estim):\n",
    "            \n",
    "            cur_grad = self.grad(cur_y_pred, y_true)\n",
    "            \n",
    "            next_estim = base_model(*self.base_args, **self.base_kwargs)\n",
    "            next_estim = next_estim.fit(X, -cur_grad)\n",
    "            self.estimators.append(next_estim)\n",
    "            \n",
    "            estim_pred = next_estim.predict(X)\n",
    "            best_coef = None\n",
    "            best_loss = 10**9\n",
    "            for c in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5]:\n",
    "                c_loss = self.loss_func(cur_y_pred + self.lr * c * estim_pred, y_true)\n",
    "                if c_loss < best_loss:\n",
    "                    best_loss = c_loss\n",
    "                    best_coef = c\n",
    "            self.coefs.append(best_coef)\n",
    "            cur_y_pred = cur_y_pred + self.lr * best_coef * estim_pred\n",
    "            \n",
    "    \n",
    "    def grad_mse(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "    \n",
    "    def grad_log(self, y_pred, y_true):\n",
    "        return -y_true/(1 + np.exp(y_true * y_pred))\n",
    "    \n",
    "    def grad_exp(self, y_pred, y_true):\n",
    "        return (-y_true * np.exp(-y_true * y_pred))\n",
    "    \n",
    "    def loss_mse(self, y_pred, y_true):\n",
    "        return (0.5*(y_pred - y_true)**2).mean()\n",
    "    \n",
    "    def loss_log(self, y_pred, y_true):\n",
    "        margin = y_pred*y_true\n",
    "        return np.exp(-margin).mean()\n",
    "    \n",
    "    def loss_exp(self, y_pred, y_true):\n",
    "        margin = y_pred*y_true\n",
    "        return (np.log(1 + np.exp(-margin))).mean()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_preds = self.estimators[0].predict(X)\n",
    "        for i in range(1, self.n_estim):\n",
    "            grads = self.lr * self.coefs[i] * self.estimators[i].predict(X)\n",
    "            y_preds += grads\n",
    "        if self.loss == 'mse':\n",
    "            return softmax(y_preds, axis = 1)\n",
    "        else:\n",
    "            return y_preds\n",
    "      \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Ваш код здесь\n",
    "        probas = self.predict_proba(X)\n",
    "        if self.loss == 'mse':\n",
    "            return probas.argmax(axis=1)\n",
    "        else:\n",
    "            return (probas > 0).astype(np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier('mse', learning_rate=0.1, n_estimators=100, colsample = 5, subsample=10)\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT STARTED\n",
      "0.8888888888888888\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train, base_model=DecisionTreeRegressor)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну теперь точно должно хотя бы на 1 балльчик схватить(пожалуйста🙏)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваш ответ здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВАШ ОТЗЫВ ЗДЕСЬ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
